# 记忆机制说明（简化版）

可以把现在的记忆系统理解成三步：先找候选记忆，再让模型决定“新增/更新/合并/连线”，最后按固定顺序写回记忆库。下面只保留必要的数学表达，不用太多专业术语。

第一步是候选检索。系统会先从输入里抽出“问题核心文本”，并只附带一小段必要上下文，尽量去掉历史记录和工具回显噪声；然后再切成关键词 $t_1,\dots,t_K$，再用分词把查询切成检索片段 $f_1,\dots,f_F$。关键词检索阶段直接用数据库做“标题包含片段”的匹配，不再先把所有记忆拉到内存再扫。每条记忆 $m$ 都会得到一个总分：

$S(m)=S_{kw}(m)+S_{rev}(m)+S_{sem}^{norm}(m)+S_{graph}(m)$。

这里 $S_{kw}$ 是“关键词命中”分，$S_{rev}$ 是“反向包含”分（查询文本直接包含记忆标题），$S_{sem}^{norm}$ 是语义相似分，$S_{graph}$ 是关系传播分。系统用这个总分排序并过滤，最后只保留前 15 条候选。

关键词命中和反向包含都用同一类排名衰减公式。设记忆重要性是 $p_m$，关键词权重是 $W_{kw}$，排名常量是 $k_0$，命中名次是 $r$。

关键词命中时，先看标题里命中了多少个片段。记 $c_m$ 为该记忆命中的片段数，$F$ 为总片段数，则有一个覆盖率增益：

$\lambda_m=1+0.6\cdot\frac{c_m}{F}$。

于是关键词阶段的单次贡献是：

$\Delta_{kw}=\frac{p_m\cdot W_{kw}}{k_0+r}\cdot\lambda_m$。

反向包含（查询文本直接包含记忆标题）的单次贡献是：

$\Delta_{rev}=\frac{p_m\cdot W_{kw}}{k_0+r}$。

语义分会先算相似度，再累加。设语义权重是 $W_{sem}$，相似度是 $sim(m,t)$，语义阈值是 $\tau$，当 $sim(m,t)\ge\tau$ 才计分。单个关键词贡献为：

$\Delta_{sem}(m,t)=\frac{\sqrt{p_m}}{k_0+r_t}+W_{sem}\cdot sim(m,t)$。

然后做长度归一化，避免“关键词多就天然高分”：

$S_{sem}^{norm}(m)=\frac{1}{\sqrt{K}}\sum_{t=1}^{K}\Delta_{sem}(m,t)$。

关系传播分的意思很直白：如果一条记忆已经高分，它连着的记忆也会加一点分。设边权为 $w_{uv}$，边传播权重为 $W_{edge}$，小常数为 $\beta$，则从 $u$ 传播到 $v$ 的增量是：

$\Delta_{graph}(u\to v)=S(u)\cdot w_{uv}\cdot W_{edge}+\beta\cdot W_{edge}$。

得到总分后会做阈值过滤。设最低通过线为 $\theta$，只有满足 $S(m)\ge\theta$ 的记忆才会留下，再按分数降序取前 15 条作为“给模型看的旧记忆”。

第二步是结构化决策。模型不会随便输出文字，而是输出结构化结果：$main,new,update,merge,links,user$。含义很简单：主事件、要新增的记忆、要更新的记忆、要合并的记忆、要建立的关系、用户偏好更新。如果模型判断“这轮没有长期价值”，就返回空对象，系统会直接结束，不写入。

第三步是写回记忆库，顺序固定：先合并，再更新，再处理主事件和新增实体，最后建关系。这个顺序的目的是减少重复和冲突。系统还会做本地高相似去重：如果新实体和已有记忆过于接近，优先复用旧记忆，不再新建。

关系建立也有硬约束：源和目标都必须能找到，而且关系要有明确证据；否则不建边。自动分类流程会把“未分类记忆”补上文件夹路径，但它只影响组织方式，不改变“该不该入库”这个决定。

最后补一句：旧问题库工具还在，但只是兼容层。真正在运行的是上面这套“检索打分 + 结构化决策 + 有序写回”的机制。
